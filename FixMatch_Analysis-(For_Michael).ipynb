{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from dataset.cifar import DATASET_GETTERS\n",
    "from utils import AverageMeter, accuracy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "best_acc = 0\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity\n",
    "import torch.nn as nn\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mu for unlabled loader is 1 currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save checkpoint funtion\n",
    "def save_checkpoint(state, is_best, checkpoint, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint,\n",
    "                                               'model_best.pth.tar'))\n",
    "\n",
    "#Setting the seed\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "# if args.seed is not None:\n",
    "#         set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch FixMatch Training')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--gpu-id', default='1', type=int,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "parser.add_argument('--num-workers', type=int, default=4,\n",
    "                    help='number of workers')\n",
    "parser.add_argument('--dataset', default='cifar10', type=str,\n",
    "                    choices=['cifar10', 'cifar100'],\n",
    "                    help='dataset name')\n",
    "parser.add_argument('--num-labeled', type=int, default=40,\n",
    "                    help='number of labeled data')\n",
    "parser.add_argument(\"--expand-labels\", action=\"store_true\",\n",
    "                    help=\"expand labels to fit eval steps\")\n",
    "parser.add_argument('--arch', default='wideresnet', type=str,\n",
    "                    choices=['wideresnet', 'resnext'],\n",
    "                    help='dataset name')\n",
    "parser.add_argument('--total-steps', default=2**20, type=int,\n",
    "                    help='number of total steps to run')\n",
    "parser.add_argument('--eval-step', default=1024, type=int,\n",
    "                    help='number of eval steps to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--batch-size', default=64, type=int,\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--warmup', default=0, type=float,\n",
    "                    help='warmup epochs (unlabeled data based)')\n",
    "parser.add_argument('--wdecay', default=5e-4, type=float,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--nesterov', action='store_true', default=True,\n",
    "                    help='use nesterov momentum')\n",
    "parser.add_argument('--use-ema', action='store_true', default=True,\n",
    "                    help='use EMA model')\n",
    "parser.add_argument('--ema-decay', default=0.999, type=float,\n",
    "                    help='EMA decay rate')\n",
    "parser.add_argument('--mu', default=7, type=int,\n",
    "                    help='coefficient of unlabeled batch size')\n",
    "parser.add_argument('--lambda-u', default=1, type=float,\n",
    "                    help='coefficient of unlabeled loss')\n",
    "parser.add_argument('--T', default=1, type=float,\n",
    "                    help='pseudo label temperature')\n",
    "parser.add_argument('--threshold', default=0.95, type=float,\n",
    "                    help='pseudo label threshold')\n",
    "parser.add_argument('--out', default='result',\n",
    "                    help='directory to output the result')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--seed', default=5, type=int,\n",
    "                    help=\"random seed\")\n",
    "parser.add_argument(\"--amp\", action=\"store_true\",\n",
    "                    help=\"use 16-bit (mixed) precision through NVIDIA apex AMP\")\n",
    "parser.add_argument(\"--opt_level\", type=str, default=\"O1\",\n",
    "                    help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                    \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                    help=\"For distributed training: local_rank\")\n",
    "parser.add_argument('--no-progress', action='store_true',\n",
    "                    help=\"don't use progress bar\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args):\n",
    "    if args.arch == 'wideresnet':\n",
    "        import models.wideresnet as models\n",
    "        model = models.build_wideresnet(depth=args.model_depth,\n",
    "                                        widen_factor=args.model_width,\n",
    "                                        dropout=0,\n",
    "                                        num_classes=args.num_classes)\n",
    "    elif args.arch == 'resnext':\n",
    "        import models.resnext as models\n",
    "        model = models.build_resnext(cardinality=args.model_cardinality,\n",
    "                                     depth=args.model_depth,\n",
    "                                     width=args.model_width,\n",
    "                                     num_classes=args.num_classes)\n",
    "    logger.info(\"Total params: {:.2f}M\".format(\n",
    "        sum(p.numel() for p in model.parameters())/1e6))\n",
    "    return model\n",
    "\n",
    "if args.local_rank == -1:\n",
    "    device = torch.device('cuda', args.gpu_id)\n",
    "    args.world_size = 1\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device('cuda', args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.n_gpu = 1\n",
    "\n",
    "args.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'cifar10':\n",
    "    args.num_classes = 10\n",
    "    if args.arch == 'wideresnet':\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 2\n",
    "    elif args.arch == 'resnext':\n",
    "        args.model_cardinality = 4\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 4\n",
    "\n",
    "elif args.dataset == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "    if args.arch == 'wideresnet':\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 8\n",
    "    elif args.arch == 'resnext':\n",
    "        args.model_cardinality = 8\n",
    "        args.model_depth = 29\n",
    "        args.model_width = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](\n",
    "    args, './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_dataset.data = np.load('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Images_New_6.npy')\n",
    "# labeled_dataset.targets = np.load('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Labels_New_6.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_images = np.load('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Images_New_6.npy')\n",
    "# check_labels = np.load('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Labels_New_6.npy')\n",
    "\n",
    "# check_images = check_images[250:]\n",
    "# check_labels = check_labels[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for i in range(len(check_images)):\n",
    "#     for j in range(len(unlabeled_dataset.data)):\n",
    "#         if (check_images[i] == unlabeled_dataset.data[j]).all():\n",
    "#             if check_labels[i] == unlabeled_dataset.targets[j]:\n",
    "#                 count+=1\n",
    "#                 break\n",
    "#             break\n",
    "        \n",
    "# count/len(check_labels)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_trainloader = DataLoader(\n",
    "    labeled_dataset,\n",
    "    sampler=train_sampler(labeled_dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "#UnLabeled Train data loader\n",
    "unlabeled_trainloader = DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    sampler=train_sampler(unlabeled_dataset),\n",
    "    batch_size=args.batch_size*args.mu,\n",
    "#     batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "\n",
    "#Test data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "model = create_model(args)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'results/cifar10@250/checkpoint.pth.tar'\n",
    "\n",
    "PATH = '/home/sumeet1/FixMatch-pytorch/results/cifar10@40.5/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = torch.Tensor([])\n",
    "test_labels = torch.Tensor([])\n",
    "def test(args, test_loader, model, epoch):\n",
    "    global test_preds\n",
    "    global test_labels\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    if not args.no_progress:\n",
    "        test_loader = tqdm(test_loader,\n",
    "                           disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            test_preds = torch.cat((test_preds, outputs.cpu().detach()), 0)\n",
    "            test_labels = torch.cat((test_labels, targets.cpu().detach()), 0)\n",
    "            \n",
    "            if not args.no_progress:\n",
    "                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    iter=len(test_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                ))\n",
    "        if not args.no_progress:\n",
    "            test_loader.close()\n",
    "\n",
    "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
    "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_ema:\n",
    "    from models.ema import ModelEMA\n",
    "    ema_model = ModelEMA(args, model, args.ema_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'bn']\n",
    "grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': args.wdecay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(grouped_parameters, lr=args.lr,\n",
    "                          momentum=0.9, nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = math.ceil(args.total_steps / args.eval_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, args.warmup, args.total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.amp:\n",
    "    from apex import amp\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=args.opt_level)\n",
    "\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[args.local_rank],\n",
    "        output_device=args.local_rank, find_unused_parameters=True)\n",
    "    \n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, test_loader, model, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    if not args.no_progress:\n",
    "        test_loader = tqdm(test_loader,\n",
    "                           disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if not args.no_progress:\n",
    "                test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    iter=len(test_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                ))\n",
    "        if not args.no_progress:\n",
    "            test_loader.close()\n",
    "\n",
    "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
    "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_iter = iter(labeled_trainloader)\n",
    "unlabeled_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.block3.layer[3].conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.block1.layer[3].bn2.register_forward_hook(get_activation('bn2'))\n",
    "output = model(next(labeled_trainloader.__iter__())[0].to(device))\n",
    "resultant = activation['bn2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_formula(pred,mu1,var1):\n",
    "    form = (1/(math.sqrt(2*(math.pi)*var1))) * (math.exp(-(pred-mu1)**2/(2*var1)))\n",
    "    return form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "test_scores = []\n",
    "label_ptr = 0\n",
    "label_ptr1 = label_ptr + 50\n",
    "\n",
    "fin_images = []\n",
    "fin_labels = []\n",
    "\n",
    "\n",
    "for epoch in range(args.start_epoch, 1024):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    mask_probs = AverageMeter()\n",
    "    if not args.no_progress:\n",
    "        p_bar = tqdm(range(args.eval_step),\n",
    "                     disable=args.local_rank not in [-1, 0])\n",
    "    \n",
    "    images_to_add = torch.Tensor([])\n",
    "    labels_to_add = torch.LongTensor([])\n",
    "    true_label = torch.LongTensor([])\n",
    "    \n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    labeled_iter = next(labeled_iter.__iter__())\n",
    "\n",
    "    for batch_idx in range(1024):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            inputs_x, targets_x = next(labeled_iter.__iter__())\n",
    "                    \n",
    "        except:\n",
    "            if args.world_size > 1:\n",
    "                labeled_epoch += 1\n",
    "                labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
    "            labeled_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x =  next(labeled_iter.__iter__())\n",
    "\n",
    "        try:\n",
    "            (inputs_u_w, inputs_u_s,norm_x), unlab_labs = next(unlabeled_iter.__iter__())\n",
    "        except:\n",
    "            if args.world_size > 1:\n",
    "                unlabeled_epoch += 1\n",
    "                unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
    "            unlabeled_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u_w, inputs_u_s,norm_x), unlab_labs = next(unlabeled_iter.__iter__())\n",
    "\n",
    "\n",
    "        batch_size = inputs_x.shape[0]\n",
    "        inputs = interleave(\n",
    "            torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*args.mu+1).to(args.device)\n",
    "        targets_x = targets_x.to(args.device)\n",
    "        logits = model(inputs)\n",
    "        logits = de_interleave(logits, 2*args.mu+1)\n",
    "        logits_x = logits[:batch_size]\n",
    "        logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "\n",
    "        Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "        pseudo_label = torch.softmax(logits_u_w.detach()/args.T, dim=-1)\n",
    "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(args.threshold).float()\n",
    "\n",
    "        \n",
    "        \n",
    "        unlab_labs = unlab_labs.to(device)\n",
    "        \n",
    "        pseudo_label1 = torch.softmax(logits_u_s.detach()/args.T, dim=-1)\n",
    "        max_probs1, targets_u1 = torch.max(pseudo_label1, dim=-1)\n",
    "        \n",
    "#         Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
    "#                               reduction='none') * mask).mean()\n",
    "        \n",
    "     \n",
    "##################################################################################################################################        \n",
    "      ############################################## Gaussian Filter PART ##############################################################\n",
    "        \n",
    "        em_mask = np.zeros(len(inputs_u_w))\n",
    "        for i in range(10):\n",
    "            indexs = np.where(targets_u.cpu().detach()==i)\n",
    "            images_to_add1 = norm_x[indexs]\n",
    "            labels_to_add1 = targets_u[indexs]\n",
    "            true_label1 = unlab_labs[indexs]\n",
    "            p1 = max_probs[indexs].cpu().detach()\n",
    "            p2 = max_probs1[indexs].cpu().detach()\n",
    "            mu1 = np.mean(np.array(p2))\n",
    "            var1 = np.var(np.array(p2))\n",
    "            fin_p2 = np.ones(p2.shape)\n",
    "            for i in range(3):\n",
    "                fin_p2 = np.ones(p2.shape)\n",
    "                for i in range(len(fin_p2)):\n",
    "                    ans = gaussian_formula(p2[i],mu1,var1)\n",
    "                    fin_p2[i]+=ans\n",
    "                mu1 = np.mean(fin_p2)\n",
    "                var1 = np.cov(fin_p2)   \n",
    "                \n",
    "#             fin_idx = np.where(fin_p2 >= np.mean(fin_p2))\n",
    "            \n",
    "            fin_idx = np.where(fin_p2 >= 0.95)\n",
    "            \n",
    "#             print(np.mean(fin_p2))\n",
    "            em_mask_pointers = indexs[0][fin_idx]\n",
    "            em_mask[em_mask_pointers]+=1\n",
    "            \n",
    "        for i in range(len(mask)):\n",
    "            if mask[i]==0 and em_mask[i]==1:\n",
    "                mask[i]+=1\n",
    "                \n",
    "        Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
    "                              reduction='none') * mask).mean()\n",
    "\n",
    "\n",
    "    ########################################################################################################################\n",
    "#     ###############################################Cosine Similarity########################################################\n",
    "        if epoch>-1:\n",
    "            if batch_idx >= label_ptr and batch_idx <= label_ptr1:\n",
    "                with torch.no_grad():\n",
    "                    model.relu.register_forward_hook(get_activation('relu'))\n",
    "                    nu_output = model(inputs_x.to(device))\n",
    "                    encoded_layer = activation['relu']\n",
    "\n",
    "\n",
    "                    model.relu.register_forward_hook(get_activation('relu1'))\n",
    "                    nu_output1 = model(inputs_u_w.to(device))\n",
    "                    encoded_layer1 = activation['relu1']\n",
    "\n",
    "\n",
    "                    r = nn.Flatten()\n",
    "                    r1 = nn.Linear(8192,256)\n",
    "\n",
    "                    l1  = r(encoded_layer.cpu().detach())\n",
    "                    l2 = r1(l1)\n",
    "\n",
    "                    l3  = r(encoded_layer1.cpu().detach())\n",
    "                    l4 = r1(l3)\n",
    "\n",
    "                    main_dict = {}\n",
    "                    ssim_loss = {}\n",
    "                    for i in range(10):\n",
    "                        main_dict[i] = []\n",
    "                        ssim_loss[i] = 0\n",
    "                    for i in range(10):\n",
    "                        dict1 = {}\n",
    "                        ssim_loss_value = 0\n",
    "                        labeled_labs = np.where(targets_x.cpu().detach()==i)[0]\n",
    "                        to_compare_pseudo_labels = np.where(targets_u.cpu().detach()==i)[0]\n",
    "                        for j in to_compare_pseudo_labels:\n",
    "                            dict1[j] = 0\n",
    "\n",
    "                        for k in labeled_labs:\n",
    "                            for l in to_compare_pseudo_labels:\n",
    "                                cosine_similarity = 1 - spatial.distance.cosine(l2[k], l4[l])\n",
    "                                if cosine_similarity >= 0.7:\n",
    "                                    dict1[l]+=1\n",
    "\n",
    "                        for m in dict1:\n",
    "                            if dict1[m]>=6:\n",
    "                                main_dict[i].append(m)\n",
    "\n",
    "\n",
    "                    for key in main_dict:\n",
    "                        images_to_add = torch.cat((images_to_add, norm_x[main_dict[key]]), 0)\n",
    "                        labels_to_add = torch.cat((labels_to_add, torch.Tensor(np.ones(len(main_dict[key]),dtype=int)*key)  ), 0)\n",
    "\n",
    "\n",
    "##################################################################################################################################        \n",
    "        \n",
    "        loss = Lx + (args.lambda_u * Lu) \n",
    "#         loss = Lx \n",
    "\n",
    "        if args.amp:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        losses_x.update(Lx.item())\n",
    "        losses_u.update(Lu.item())\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if args.use_ema:\n",
    "            ema_model.update(model)\n",
    "        model.zero_grad()\n",
    "\n",
    "        mask_probs.update(mask.mean().item())\n",
    "\n",
    "    if not args.no_progress:\n",
    "        p_bar.close()\n",
    "\n",
    "    if args.use_ema:\n",
    "        test_model = ema_model.ema\n",
    "    else:\n",
    "        test_model = model\n",
    "        \n",
    "##################################################################################################################################    \n",
    "    \n",
    "    \n",
    "    if len(labels_to_add)>0:\n",
    "        for i in range(len(images_to_add)):\n",
    "            flag = 0\n",
    "            for j in range(len(labeled_dataset.data)):\n",
    "                if (np.asarray(images_to_add[i])==labeled_dataset.data[j]).all():\n",
    "                    flag+=1\n",
    "            if flag==0:\n",
    "                fin_images.append(np.asarray(images_to_add[i]))\n",
    "                fin_labels.append(labels_to_add[i])\n",
    "                \n",
    "        print('Length of Fin_Images',len(fin_images))\n",
    "        \n",
    "        if len(np.unique(np.array(fin_labels)))==10:\n",
    "            \n",
    "            fin_images = np.array(fin_images)\n",
    "            fin_labels = np.array(fin_labels)\n",
    "\n",
    "            final_images_to_add = torch.Tensor([])\n",
    "            final_labels_to_add = torch.LongTensor([])\n",
    "\n",
    "\n",
    "            labels_to_add_pd = pd.DataFrame(fin_labels,dtype=int)\n",
    "            labels_to_add_pd_min = labels_to_add_pd[0].value_counts().min()\n",
    "\n",
    "\n",
    "            for i in range(10):\n",
    "                reference = np.where(fin_labels==i)\n",
    "                reference1 = reference[0][:labels_to_add_pd_min]\n",
    "                final_images_to_add =   torch.cat((final_images_to_add,torch.Tensor(fin_images[reference1])),0)\n",
    "                final_labels_to_add =   torch.cat((final_labels_to_add,torch.LongTensor(fin_labels[reference1])),0)\n",
    "\n",
    "            final_images_to_add = np.array(final_images_to_add,dtype=np.uint8)\n",
    "            final_labels_to_add = np.array(final_labels_to_add)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #################### Checking for confounding error rate in every epoch#######################\n",
    "            \n",
    "            error_rate_count = 0\n",
    "            for z in range(len(final_images_to_add)):\n",
    "                for s in range(len(unlabeled_dataset.targets)):\n",
    "                    if (np.asarray(final_images_to_add[z])==unlabeled_dataset.data[s]).all():\n",
    "                        if final_labels_to_add[z] == unlabeled_dataset.targets[s]:\n",
    "                            error_rate_count+=1\n",
    "                            break\n",
    "                        break\n",
    "                            \n",
    "            print('Error_Rate_Accuracy', error_rate_count/len(final_images_to_add))\n",
    "            \n",
    "            ###############################################################################################\n",
    "            \n",
    "            \n",
    "\n",
    "            labeled_dataset.data = np.concatenate((labeled_dataset.data,final_images_to_add),axis = 0)\n",
    "            labeled_dataset.targets = np.concatenate((labeled_dataset.targets,final_labels_to_add),axis = 0)\n",
    "\n",
    "            print('labeled_dataset.data.shape',labeled_dataset.data.shape)\n",
    "            \n",
    "#             np.save('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Images_New_6',labeled_dataset.data)\n",
    "#             np.save('/xace/d2/sumeet/Fix_Match_Data/Labeled_Dataset_Labels_New_6',labeled_dataset.targets)\n",
    "\n",
    "            labeled_trainloader = DataLoader(\n",
    "            labeled_dataset,\n",
    "            sampler=train_sampler(labeled_dataset),\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=True)\n",
    "            \n",
    "            fin_images = []\n",
    "            fin_labels = []\n",
    "\n",
    "#################################################################################################################################    \n",
    "    print('Lx',Lx)\n",
    "    print('Lu',Lu)\n",
    "    print('Lambda:',args.lambda_u)\n",
    "    test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
    "    print('Test_Accuracy',test_acc)\n",
    "    test_scores.append(test_acc)\n",
    "    \n",
    "    if epoch>-1:\n",
    "        label_ptr+=50\n",
    "        label_ptr1 = label_ptr + 50\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
