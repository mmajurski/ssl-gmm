
%\documentclass[10pt,twocolumn]{article} 
\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[review]{cvpr}
\usepackage{cvpr}

%\usepackage{simpleConference}

%\usepackage[margin=1in]{geometry}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{wrapfig}


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\def\confName{ar$\xi$iv}
\def\confYear{2023}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
	pdftitle={Beyond the Final Linear Layer},
	pdfauthor={Michael Majurski, David Chapman},
	pdfkeywords={AI,Semi-Supervised,Classification,FixMatch},
}



\begin{document}

\title{Beyond the Final Linear Layer: Enhancing Decision Boundaries}

\author{Michael Majurski\\
	Information Technology Lab, NIST\\
%	100 Bureau Dr. Gaithersburg MD, 20899\\
	{\tt\small michael.majurski@nist.gov}
	% For a paper whose authors are all at the same institution,
	% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Sumeet Menon\\
University of Maryland, Baltimore County\\
\and
David Chapman\\
University of Maryland, Baltimore County\\
}
\maketitle








	
	
\maketitle


\begin{abstract}
% TODO rewrite abstract once paper is done to fill in details
SSL leverages an abundance of unlabeled data to improve deep learning based model performance under limited training data regimes.
This paper presents a novel extension to any image classification architecture which improves accuracy in low-label regimes. 
We extend the FixMatch \cite{sohn2020fixmatch} training scheme with our novel last layers and demonstrate test accuracy improvement. 
The novelty consists of 2 elements: first we replace the last linear layer with a GMM trained via backprop, and second, we impose class-wise constraints on the embedding space the GMM operates on.
These methods match published SOTA 250 label CIFAR-10 \cite{cifar10} results and come close to matching SOTA in the 40 label regime without the significant model complexity of methods like SimMatchV2 \cite{zheng2023simmatchv2}.
Our method achieves 94.8\% and 94.2\% accuracy with 250 and 40 CIFAR-10 labels respectively.
Our code is available at: \url{https://github.com/mmajurski/ssl-gmm}  % TOOD cleanup the repo
\end{abstract}


\section{Introduction}
SSL leverages an abundance of unlabeled data to improve deep learning based model performance under limited training data regimes \cite{zhu2022introduction,li2019safe,hady2013semi}.
Image classification has become a playground for exploring new SSL ideas.
The early successes of deep learning based methods relied on large annotated datasets to enable models to learn the relevant features to perform the task, i.e. image classification build on top of ImageNet \cite{deng2009imagenet}.
With data annotation becoming a significant bottleneck, especially in application domains outside of the standard benchmarks, another learning paradigm was needed.

There are several flavors of SSL.
Contrastive learning methods leverage the intuition that similar instances should be close in the representation space, while different instances are farther apart \cite{yang2022class,li2021comatch}.
Consistency regularization borrows the intuition that modified views of the same instance should have similar representations and predictions \cite{sohn2020fixmatch,lee2022contrastive,zhang2021flexmatch,kim2022conmatch}.
Pseudo-labeling methods like FixMatch \cite{sohn2020fixmatch} fall within the consistency regularization domain.

This work argues that pseudo-labeling methods can be improved with better calibration of the network logits used to filter the pseudo-labels into reliable and unreliable. 
Neural networks are known to be overconfident in their predictions \cite{wei2022mitigating}, and this affects the pseudo-labeling process. 
Potentially allowing for the inclusion of more incorrect pseudo-labels any specific logit threshold would otherwise have.
This work demonstrates the better calibrated replacements for a models final linear layer can improve the final accuracy of pseudo-labeling based SSL algorithms in very label scarce regimes. 

With that goal in mind, this work proposes replacing the final linear (fully connected) layer of the neural network with one of a few options, all of which have explicit modeling of class cluster centroids.
First, a nearest neighbor (k=1) kmeans layer, which has been used in prior work \cite{dwibedi2021little}.
Second, an axis-aligned  differentiable Gaussian Mixture Model (GMM) trained via back prop.
Third, an identity covariance axis-aligned differentiable GMM trained via back prop. 
These three methods represent differing levels of prescription about how the final embedding space should be arranged.
The idea of leveraging clusters in embedding space is not new \cite{caron2018deep,caron2020unsupervised}, but we extend the core idea with a novel differentiable model of learned cluster centroids. 

% TODO write up what the motivation for our method was, why do the things we do...? (not because they are easy, but because they are hard...) Most papers have an inciting incident in the intro which outlines the hypothesis that X is a problem, and we addressed that via Y to improve the results by Z.

Additionally, to assist these cluster based final embedding classification layers, we explore the effect of different constraints on the embedding space.
All of the embedding space constraints are based on specifying how many of the first 4 gaussian moments are identity/zero.
Moment1 (cluster mean position) should have zero distance to the learned cluster centroids. % TODO (chapman) need citation for guassian moments
Moment2 (cluster variance) should be the identity matrix.
Moment3 (cluster skew) should be zero.
Moment4 (cluster kurtosis) should be the identity matrix. 
We explore how no constraints, moment1, moment1-2, and moment1-4 impact the final learned model accuracy.

This paper contributes a simple easy to implement improvement to pseudo-labeling methods where very few annotations are available, replace the last linear layer with a kmeans layer which explicitly models class cluster centers.
We demonstrate this methodology using CIFAR-10 and CIFAR-100 \cite{cifar10} with 40 and 400 labels respectively. 
Additionally, we explore and demonstrate that high level prescriptive constraints on the embedding space produce significantly worse outcomes than allowing the embedding space to take on whatever emergent structure the training process produces. 
Finally, because the embedding constraint penalties are applied to all unlabeled data and not just the valid pseudo-labels, our method extracts training signal from every unlabeled data point, unlike FixMatch \cite{sohn2020fixmatch} and other methods which only learn from the valid pseudo-labels.




% TODO CE masked where any truely low logit values are kept, but those midling are left alone.? I.e keep CE <0.1 but ignore those 0.1 < x < 0.95. cite chen2023boosting and rizve2021defense and kim2019nlnl for the idea. I.e. keep the strong negative PL in the CE term. chen2023boosting uses topk, instead of thresholds. 

% TODO perform study with best mothod (i.e. aa_gmm_d1 or kmeans with l2) and show impact of PL threshold and the model impurity rate compared to baseline fixmatch (fc). Gerenate plot of impurity for FM vs gmm over the first 100-200 epochs?

% TODO build t-SNE plots of the embedding spaces for the best models (1 per configuration)


%PyTorch \cite{pytorch} 

% right now our method is a combination of 
% - last layer replacement fc -> GMM
% - embedding constraint l2
% - null result that fancy embedding constraints help (mean_covar is worse performing)
% - (maybe) that negative sampling helps a la chen2023boosting and rizve2021defense
% - TODO ablation study on the above

% TODO test more potent clustering method in place of l2 kenyon2018clustering


\section{Related Work}


% TODO compare our method to lee2022contrastive which uses explicit cluster centers, and draws both the valid and non valid PL to the cluster center, just like we do, we just don't have a contrastive element like they do

Semi-Supervised learning has recently show great progress in learning high quality models, in some cases match fully supervised performance, for a number of benchmarks \cite{zhang2021flexmatch}.
The goal of SSL is to produce a trained model of equivalent accuracy to fully supervised training, with vastly reduced data annotation requirements.

\subsection{Pseudo-Labeling}
Self-supervised learning was among the initial approaches employed in the context of semi-supervised learning to annotate unlabeled images. 
This technique involves the initial training of a classifier with a limited set of labeled samples and incorporates pseudo-labels into the gradient descent process, exceeding a predefined threshold \cite{yarowsky1995unsupervised, mcclosky2006reranking, olivier2006semi,zhai2019s4l,livieris2019predicting,rosenberg2005semi}. 
A closely related method to self-training is co-training, where a given dataset is represented as two distinct feature sets \cite{blum1998combining}. 
These independent sample sets are subsequently trained separately using two distinct models, and the sample predictions surpassing predetermined thresholds are utilized in the final model training process \cite{blum1998combining,prakash2014survey}.
A notably advanced approach to pseudo-labeling is the Mean Teacher algorithm \cite{tarvainen2017mean}, which leverages exponential moving averages of model parameters to acquire a notably more stable target prediction. 
This refinement has a substantial impact on enhancing the convergence of the algorithm.


\subsection{Consistency Regularization}

Consistency regularization operates on the premise that when augmenting an unlabeled sample, its label should remain consistent. 
This approach implicitly enforces a smoothness assumption, promoting coherence between unlabeled samples and their basic augmentations \cite{xie2020unsupervised}. 
In other words, the model should be able to predict the unlabeled sample x exactly the same way it predicts the class for Augmented(x) \cite{berthelot2019mixmatch,sohn2020fixmatch,berthelot2019remixmatch,mustafa2020transformation}. 
In addition to evaluating image-wise augmentations, recent research has demonstrated that incorporating class-wise and instance-based consistencies yields superior performance outcomes \cite{zheng2022simmatch,li2021comatch}.
Finally, pseudo-labeling filtering based on consistence between strongly augmented views shows convergence improvement \cite{kim2022conmatch}.

\subsection{Embedding Clustering/Constraints}

% TODO talk about the paper which find linear layers end up being nearest neighbor assignment (with cone away from origin structure) in the embedding space

Several papers have attempted to improve the quality of pseudo-labels to either improve the final model accuracy, improve the rate of convergence, or avoid confirmation bias \cite{arazo2020pseudo}.
Rizve et al. \cite{rizve2021defense} explores how uncertainty aware pseudo-label selection/filtering can be used to reduce the label noise.
Incorrect pseudo-labels can be viewed as a network calibration issue \cite{rizve2021defense} where better network logit calibration might improve results \cite{Xing2020DistanceBased}.
Other work has attempted to improve the pseudo-labeling process by imposing curriculum \cite{zhang2021flexmatch} or by including a class-aware contrastive term \cite{yang2022class}.
Previous work has leveraged the concept of explicit class cluster centers for conditioning semantic similarity \cite{zheng2022simmatch}.
Recent work has extended purely clustering based methods like DINO \cite{caron2021emerging} into semi-supervised methods \cite{fini2023semi}.





\section{Methodology}

In this section, we explore the details of our proposed embedding constraints and final linear layer replacements.

\subsection{Fully Connected Replacements}

\subsubsection{KMeans}

\subsubsection{Differentiable Axis Aligned Gaussian Mixture Model}


\subsection{Embedding Constraints}

\subsubsection{L2}

\subsubsection{mean/covar}






\section{Experiments}


\subsection{Ablation Study}



\section{Conclusions}





\bibliographystyle{unsrt}
\bibliography{refs}
	
	
\end{document}