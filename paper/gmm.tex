\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[review]{cvpr}
%\usepackage[final]{cvpr}
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{graphicx}
\usepackage{wrapfig}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


%
% --- inline annotations
%
\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
% --- disable by uncommenting  
% \renewcommand{\TODO}[1]{}
% \renewcommand{\todo}[1]{#1}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\def\confName{ar$\xi$iv}
\def\confYear{2023}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
	pdftitle={Beyond the Final Linear Layer},
	pdfauthor={Michael Majurski, David Chapman},
	pdfkeywords={AI,Semi-Supervised,Classification,FixMatch},
}



\begin{document}
	
\title{Beyond the Final Linear Layer: Enhancing Decision Boundaries}

\author{Michael Majurski\\
	Information Technology Lab, NIST\\
	%	100 Bureau Dr. Gaithersburg MD, 20899\\
	{\tt\small michael.majurski@nist.gov}
	% For a paper whose authors are all at the same institution,
	% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Sumeet Menon\\
University of Maryland, Baltimore County\\
\and
David Chapman\\
University of Miami\\
}

\maketitle


\begin{abstract}
\TODO {(majurski) rewrite abstract once paper is done to fill in details}
SSL leverages an abundance of unlabeled data to improve deep learning based model performance under limited training data regimes.
This paper presents a novel extension to any image classification architecture which improves accuracy in low-label regimes. 
We extend the FixMatch \cite{sohn2020fixmatch} training scheme with our novel last layers and demonstrate test accuracy improvement. 
The novelty consists of 2 elements: first we replace the last linear layer with a GMM trained via backprop, and second, we impose class-wise constraints on the embedding space the GMM operates on.
These methods match published SOTA 250 label CIFAR-10 \cite{cifar10} results and come close to matching SOTA in the 40 label regime without the significant model complexity of methods like SimMatchV2 \cite{zheng2023simmatchv2}.
Our method achieves 94.8\% and 94.2\% accuracy with 250 and 40 CIFAR-10 labels respectively.
\TODO {cleanup the repo}
Our code is available at: \url{https://github.com/mmajurski/ssl-gmm}
\end{abstract}


\section{Introduction}
SSL leverages an abundance of unlabeled data to improve deep learning based model performance under limited training data regimes \cite{zhu2022introduction,li2019safe,hady2013semi}.
Image classification has become a playground for exploring new SSL ideas.
The early successes of deep learning based methods relied on large annotated datasets to enable models to learn the relevant features to perform the task, i.e. image classification build on top of ImageNet \cite{deng2009imagenet}.
With data annotation becoming a significant bottleneck, especially in application domains outside of the standard benchmarks, another learning paradigm was needed.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\columnwidth]{example-image-a}
	\caption{High level overview of our method. \TODO {(majurski) Create this figure.}}
	\label{fig:schema}
\end{figure}

There are several flavors of SSL.
Contrastive learning methods leverage the intuition that similar instances should be close in the representation space, while different instances are farther apart \cite{yang2022class,li2021comatch}.
Consistency regularization borrows the intuition that modified views of the same instance should have similar representations and predictions \cite{sohn2020fixmatch,lee2022contrastive,zhang2021flexmatch,kim2022conmatch}.
Pseudo-labeling methods like FixMatch \cite{sohn2020fixmatch} fall within the consistency regularization domain.

This work argues that pseudo-labeling methods can be improved with better calibration of the network logits used to filter the pseudo-labels into reliable and unreliable. 
Neural networks are known to be overconfident in their predictions \cite{wei2022mitigating}, and this affects the pseudo-labeling process. 
Potentially allowing for the inclusion of more incorrect pseudo-labels any specific logit threshold would otherwise have.
This work demonstrates the better calibrated replacements for a models final linear layer can improve the final accuracy of pseudo-labeling based SSL algorithms in very label scarce regimes. 
This work proposes:

% TODO write up what the motivation for our method was, why do the things we do...? (not because they are easy, but because they are hard...) Most papers have an inciting incident in the intro which outlines the hypothesis that X is a problem, and we addressed that via Y to improve the results by Z.
\begin{enumerate}
	\item Replacing the final linear (fully connected) layer of the neural network with either kmeans \cite{dwibedi2021little} or axis-aligned differentiable Gaussian Mixture Model (GMM) trained via back prop, both of which have explicit modeling of class cluster centroids. 
	\item We explore various constraints on how the embedding space should be structured by adding penalties if the per-class clustering does not conform to between 0 and 4 of the first gaussian moments being identity/zero.
	\item We demonstrate that increasing the specificity of how the embedding space should be structure negatively impacts model performance. \TODO {(chapman) need citation for guassian moments}
\end{enumerate}

This paper contributes a simple easy to implement improvement to pseudo-labeling methods where very few annotations are available, replace the last linear layer with a kmeans layer which explicitly models class cluster centers.
We demonstrate this methodology using CIFAR-10 and CIFAR-100 \cite{cifar10} with 40 and 400 labels respectively. 
Additionally, we explore and demonstrate that high level prescriptive constraints on the embedding space produce significantly worse outcomes than allowing the embedding space to take on whatever emergent structure the training process produces. 
Finally, because the embedding constraint penalties are applied to all unlabeled data and not just the valid pseudo-labels, our method extracts training signal from every unlabeled data point, unlike FixMatch \cite{sohn2020fixmatch} and other methods which only learn from the valid pseudo-labels.


% TODO CE masked where any truely low logit values are kept, but those midling are left alone.? I.e keep CE <0.1 but ignore those 0.1 < x < 0.95. cite chen2023boosting and rizve2021defense and kim2019nlnl for the idea. I.e. keep the strong negative PL in the CE term. chen2023boosting uses topk, instead of thresholds. 

% TODO perform study with best mothod (i.e. aa_gmm_d1 or kmeans with l2) and show impact of PL threshold and the model impurity rate compared to baseline fixmatch (fc). Gerenate plot of impurity for FM vs gmm over the first 100-200 epochs?

\TODO {(majurski) build t-SNE plots of the embedding spaces for the best models (1 per configuration)}

\TODO {(majurski) purge bibliography of arxiv pre-prints where possible, replacing with their peer reviewed equivalents}


%PyTorch \cite{pytorch} 

% right now our method is a combination of 
% - last layer replacement fc -> GMM
% - embedding constraint l2
% - null result that fancy embedding constraints help (mean_covar is worse performing)
% - (maybe) that negative sampling helps a la chen2023boosting and rizve2021defense
% - TODO ablation study on the above

% TODO test more potent clustering method in place of l2 kenyon2018clustering


\section{Related Work}


% TODO compare our method to lee2022contrastive which uses explicit cluster centers, and draws both the valid and non valid PL to the cluster center, just like we do, we just don't have a contrastive element like they do

Semi-Supervised learning has recently show great progress in learning high quality models, in some cases match fully supervised performance, for a number of benchmarks \cite{zhang2021flexmatch}.
The goal of SSL is to produce a trained model of equivalent accuracy to fully supervised training, with vastly reduced data annotation requirements.

\subsection{Pseudo-Labeling}
Self-supervised learning was among the initial approaches employed in the context of semi-supervised learning to annotate unlabeled images. 
This technique involves the initial training of a classifier with a limited set of labeled samples and incorporates pseudo-labels into the gradient descent process, exceeding a predefined threshold \cite{yarowsky1995unsupervised, mcclosky2006reranking, olivier2006semi,zhai2019s4l,livieris2019predicting,rosenberg2005semi}. 
A closely related method to self-training is co-training, where a given dataset is represented as two distinct feature sets \cite{blum1998combining}. 
These independent sample sets are subsequently trained separately using two distinct models, and the sample predictions surpassing predetermined thresholds are utilized in the final model training process \cite{blum1998combining,prakash2014survey}.
A notably advanced approach to pseudo-labeling is the Mean Teacher algorithm \cite{tarvainen2017mean}, which leverages exponential moving averages of model parameters to acquire a notably more stable target prediction. 
This refinement has a substantial impact on enhancing the convergence of the algorithm.


\subsection{Consistency Regularization}

Consistency regularization operates on the premise that when augmenting an unlabeled sample, its label should remain consistent. 
This approach implicitly enforces a smoothness assumption, promoting coherence between unlabeled samples and their basic augmentations \cite{xie2020unsupervised}. 
In other words, the model should be able to predict the unlabeled sample x exactly the same way it predicts the class for Augmented(x) \cite{berthelot2019mixmatch,sohn2020fixmatch,berthelot2019remixmatch,mustafa2020transformation}. 
In addition to evaluating image-wise augmentations, recent research has demonstrated that incorporating class-wise and instance-based consistencies yields superior performance outcomes \cite{zheng2022simmatch,li2021comatch}. 
Similarly, using consistencies between augmentations, of the predictions and low-dimensional embeddings of the strong and weak augmentations of the unlabeled images in a graph based setup has shown improvement over class-wise and instance-based consistencies \cite{zheng2023simmatchv2}.
Finally, pseudo-labeling filtering based on consistence between strongly augmented views shows convergence improvement \cite{kim2022conmatch}.

\subsection{Embedding Clustering/Constraints}

% TODO talk about the paper which find linear layers end up being nearest neighbor assignment (with cone away from origin structure) in the embedding space (Majurski cannot find the citation, so anyone who remembers what this paper was titled please leave a comment here.

Several papers have attempted to improve the quality of pseudo-labels to either improve the final model accuracy, improve the rate of convergence, or avoid confirmation bias \cite{arazo2020pseudo}.
Rizve et al. \cite{rizve2021defense} explores how uncertainty aware pseudo-label selection/filtering can be used to reduce the label noise.
Incorrect pseudo-labels can be viewed as a network calibration issue \cite{rizve2021defense} where better network logit calibration might improve results \cite{Xing2020DistanceBased}.
Other work has attempted to improve the pseudo-labeling process by imposing curriculum \cite{zhang2021flexmatch} or by including a class-aware contrastive term \cite{yang2022class}.
Previous work has leveraged the concept of explicit class cluster centers for conditioning semantic similarity \cite{zheng2022simmatch}.
Recent work has extended purely clustering based methods like DINO \cite{caron2021emerging} into semi-supervised methods \cite{fini2023semi}.

% TODO talk about SimMatch embedding constraint via semantic seimilarity?



\section{Methodology}

In this section, we explore our proposed final layer replacements and embedding space constraints.
FixMatch \cite{sohn2020fixmatch} is a simple, well performing, SSL algorithm.
As such it serves as a good comparison point for exploring the effect of our contributions.
Our methodology is based upon the published FixMatch \cite{sohn2020fixmatch} algorithm, with identical hyper-parameters unless otherwise stated.
We add to FixMatch an early-stopping condition when the model has not improved for 40 epochs (where epoch size is 1024 batches as in FixMatch), with 2 learning rate reductions by a factor of $0.2$ instead of configuring a fixed number of training epochs with a cosine learning rate decay.
Additionally, we employ a cyclic learning rate scheduler to vary the learning rate by a factor of $\pm2.0$ within each epoch to make training less dependent upon exact learning rate value.

Both the linear layer replacements and the embedding constraints explored herein represent increasing levels of prescription about how the final embedding space should be arranged compared to a final linear layer.
The idea of leveraging clusters in embedding space is not new \cite{caron2018deep,caron2020unsupervised}, but we extend the core idea with a novel differentiable model of learned cluster centroids and GMMs.

\subsection{Alternative Final Layers}

A limitation of traditional final activation layers such as softmax is that they are fully discriminative; i.e. they estimate the posterior $p(Y|X)$, but do not attempt to model the sample distribution $p(X)$ or the joint probabilities $p(Y,X)$. 
To overcome this limitation, we present two semi-parametric final activation layers (a) the Axis Aligned GMM (AAGMM) layer, and (b) an equal variance version of AAGMM that we henceforth call the KMeans activation layer due to the similarity of the objective function with a gradient based KMeans.

These activation layers are fully differentiable and integrated into the neural network architecture as a module in the same way as a traditional final linear layer. 
As such, they do not require or depend on external training and do not use expectation maximization.
They are drop in replacements for the final linear layer.

Importantly, these activation layers exhibit both discriminative and generative properties. 
The neural network model $F(X;\theta_F)$ transforms the data $X$ into a latent space $Z = F(X;\theta_F)$, and the final activation layer estimates the probability densities $p(X)$, $p(Y;X)$ and $p(Y|X)$ by fitting a parametric model to the latent representation $Z$.

\subsubsection{Axis Aligned Gaussian Mixture Model Layer}

The AAGMM layer defines a set of $K$ trainable clusters, one cluster per label category. 
Each cluster $k=1 \dots K$ has a cluster center $\mu_k$ and cluster covariance $\Sigma_k$. 
The prior probability of any given sample $X_i$ is defined by the mixture of cluster probability densities over the latent representation $Z_i$ as follows,

\begin{equation}
	\begin{aligned}
		\label{eq_px}
		&p(X_i) = \sum_{k=1}^K \mathcal{N} (Z_i, \mu_{k}, \Sigma_k)
		\\[10pt]
		&\textit{where} \quad Z_i = F(X_i, \theta_F)
	\end{aligned}
\end{equation}

Where $\mathcal{N}(Z_i, \mu_k, \Sigma_k)$ represents the multivariate gaussian pdf with centroid $\mu$ and covariance $\Sigma_k$. 
AAGMM is axis aligned because $\Sigma_k$ is a diagonal matrix, as such the Normal pdf simplifies to the joint density of the pdfs along each of the $D$ axes as follows,

\begin{equation}
	\begin{aligned}
		\mathcal{N} (X_i, \mu_{k}, \Sigma_k) &=  \prod_{d=1}^D \frac{1}{\sigma_{k,d}\sqrt{2 \pi}} exp \Big( \frac{Z_{i,d} - \mu_{k,d}} {\sigma_{k,d}} \Big)^2 \\[10pt]
		&\textit{where} \quad \sigma^2_{k,d} = \Sigma_{k,d,d}
	\end{aligned}
\end{equation}

As there is one cluster per label category, the joint probability for sample $i$ with label assignment $k$, $p(Y_k,X_i)$ is the given by the normal pdf of the $k^{th}$ cluster,

\begin{equation}
	\label{eq_pyx}
	p(Y_{i,k},X_i) = \mathcal{N} (Z_i, \mu_{k}, \Sigma_k) \end{equation}

By simple Bayesian identity, the posterior probability $\hat{Y}_k=p(Y_{k}|X_i)$ can therefore be inferred from eq \ref{eq_px} and \ref{eq_pyx} as follows,

\begin{equation}
	\hat{Y}_{i,k} = p(Y_{i,k}|X_i) = \frac{p(Y_{i,k}, X_i)}{p(X_i)}
\end{equation}

%The $\mu_k$ and $\Sigma_k$ are trainable parameters and are trained as part of the gradient descent procedure.  The posterior probabilities $\hat{Y}_{i,c} = p(Y_i=c|X_i)$ are calculated by making use of a Bayesian identity $p(b|a) = p(b,a)/p(a)$, along with mutual exclusion of the label categories as follows,
%
%\begin{equation}
%p(Y_i=c|X_i) = \frac{p(Y_i=c,X_i)}{\sum_{j=1}^C}
%\end{equation}

%as follows,
%
%\begin{equation}
%\begin{aligned}
%\mathcal{N} (X_i, \mu_{k}, \Sigma_d) = \\
%(d \pi)^{- \frac{d}{2}}det(\Sigma_k)^{- \frac{1}{2}} \\
%exp\Big(-\frac{1}{2}(X_i-\mu_k)^t \Sigma^{-1} (X_i-\mu_k)\Big)
%\end{aligned}
%\end{equation}

\subsubsection{KMeans Layer}

The KMeans final layer is a more restrictive form of the AAGMM layer, in the sense that we impose an additional constraint that the gaussian covariance matrix $\Sigma_k$ for each cluster center $k$ is the $[D \times D]$ identity matrix. 
This constraint yields spherical cluster centers similar to how the traditional KMeans algorithm also assumes spherical clusters.
%It is notable, that in both cases, the AAGMM layer and the KMeans layer are not trained using the KMeans algorithm, or with any form of expectation maximization.  Rather, they are trained by gradient descent as a fully integrated module within the network architecture.  

%The KMeans activation layer defines a set of C trainable cluster centers, one cluster center per label category.  This layer is called a KMeans layer because, like KMeans, it makes use of equal-variance gaussians to define the prior $p(X)$.  However, it is not trained using the KMeans algorithm, but instead by making use of cross entropy loss as part of the neural network model.  We define $K$ $D$-dimensional cluster centers $\mu_1 \ldots \mu_K$, one center per label category.  This constitutes a generative model, where the prior probability of any given sample $X_i$ is defined in terms of the cluster centers as follows

\subsection{Semi-parametric Embedding Constraints}

We introduce and evaluate a series of embedding constraints based on the Method of Moments (MoM). 
%MoM constraints typically use the L2 norm to constrain the.  % TODO clarification for this sentence?
As our goal is semi-supervised classification, the primary goal is to minimize the expected difference between $Y$ and $\hat{Y}$ through cross entropy loss. 
However, if one were to train the model without any embedding constraints, then it is possible that the model could learn a good decision boundary for the posteriors $p(Y|X)$ but without actually modeling the probability of samples $p(X)$.
The first four gaussian moments being identity/zero increasingly constrain the model latent embedding space. 

%Define $Z^k$, $X^k$, $Y^k$, and $\hat{Y}^k$, as the latents, samples, labels, and posteriors for the elements that are predicted to reside with label $Y^k$ as follows,
%
%\begin{equation}
%(Z^k, X^k, Y^k, \hat{Y}^k) = \{(Z_i, X_i, Y_i, \hat{Y}_i) \; \; | \; \;  \underset{\rm j}{\rm argmax} \; \hat{Y}_{i,j}=k \}
%\end{equation}

\subsubsection{Moment1}

This constraint encourages the learned cluster means to have zero distance to the ovserved centroids. 
Define $pred_k$ as the set of indices $i=1 \dots K$ such that are predicted to be of category $k$ as follows,

\begin{equation}
	pred_k = \{i \;\; | \;\; \underset{j}{argmax} \; \hat{Y}_{i,j} \; \text{is equal to} \; k\}
\end{equation}

This embedding constraint ensures that cluster centers $\mu_k$ approximate the sample distribution of latent points that are part of cluster $k$, $E(Z_i|\hat{Y}_{i,k}=1)$. 
The $L2$ norm is used as a loss function to constrain cluster centers as follows,

\begin{equation}
	\begin{aligned}
		\mathcal{L}_{L2} = \sum_{k=1}^K \Bigm\lvert\Bigm\lvert \mu_k - \bar{Z} \Bigm\lvert\Bigm\lvert_2 \\[5pt]
		\textit{where} \quad \bar{Z} = E_{i \in pred_k}Z_i
	\end{aligned}
\end{equation}

\subsubsection{Moment2}

Additionally, the L2 norm was also used to constraint the cluster covariance terms to the identity matrix.
Although the AAGMM assumes a diagonal covariance matrix $\Sigma_k$, the mini-batch sample estimate of the covariance may exhibit off-diagonal terms, which represent

\subsubsection{Moments 3 and 4}

Moment3 encourages clusters to have zero skew. 
% TODO {fill in with math if required? Or just remove the subsubsections and talk about all the moments in a few paragraphs}

Moment4 encourages cluster kurtosis to be the identity matrix. 
% TODO {fill in with math if required? Or just remove the subsubsections and talk about all the moments in a few paragraphs}

\TODO {figure out how to talk about the moments 3 and 4, since all of our testing indicates that the GPU memory requirement is horrifying, and they are unstable as hell. As such they are not useful but as a baseline to compare the feasibility.}





\section{Experiments}

We evaluate our drop in final linear layer replacements on common SSL 

\TODO {(majurski) continue writing here}


\subsection{Ablation Study}



\section{Conclusions}





{
	\small
	\bibliographystyle{ieeenat_fullname}
	\bibliography{refs}
}


\end{document}